{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "03_eda_verification.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tchih11/qiita_eda/blob/main/notebooks/03_eda_verification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LngtB6qtPxeB"
      },
      "source": [
        "# 各種インストール"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BS1d3oIvQM6F"
      },
      "source": [
        "!git clone https://tchih11:@github.com/tchih11/qiita_eda.git"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xgZgOmhPZMx7"
      },
      "source": [
        "# early stopping 用\n",
        "import os\n",
        "!git clone https://github.com/Bjarten/early-stopping-pytorch.git\n",
        "os.rename('early-stopping-pytorch','early_stopping_pytorch')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q3GBfl75oVnh"
      },
      "source": [
        "%%capture\n",
        "!pip install transformers==3.5.1\n",
        "!pip install fugashi\n",
        "!pip install ipadic\n",
        "\n",
        "# エラー回避のため、バージョン変更。実行後ランタイムを再起動する。\n",
        "!pip install torch==1.4.0"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bnRfgXSw1Y2p"
      },
      "source": [
        "import os\n",
        "import random\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "from early_stopping_pytorch.pytorchtools import EarlyStopping\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from tqdm import tqdm\n",
        "from transformers import BertForSequenceClassification\n",
        "from transformers.tokenization_bert_japanese import BertJapaneseTokenizer\n",
        "\n",
        "SEED_VALUE = 0  # これはなんでも良い\n",
        "os.environ['PYTHONHASHSEED'] = str(SEED_VALUE)\n",
        "random.seed(SEED_VALUE)\n",
        "np.random.seed(SEED_VALUE)\n",
        "torch.manual_seed(SEED_VALUE)  # PyTorchを使う場合"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tXdJVhgYq9Z1"
      },
      "source": [
        "pretrained_model = 'cl-tohoku/bert-base-japanese-char-whole-word-masking'\n",
        "tokenizer = BertJapaneseTokenizer.from_pretrained(pretrained_model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yLIMfUDFTOwL"
      },
      "source": [
        "# 関数の定義\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AVJOTREx9_4j"
      },
      "source": [
        "def tokenizer_512(input_text):\n",
        "    \"\"\"\n",
        "    文章をtokenizeしてpytorchのTensorに変換\n",
        "\n",
        "    Args:\n",
        "        input_text (str): tokenizeしたい文章\n",
        "\n",
        "    Returns:\n",
        "        Tensor\n",
        "    \"\"\"    \n",
        "    return tokenizer.encode_plus(\n",
        "                    input_text,                      \n",
        "                    add_special_tokens = True,\n",
        "                    max_length = max_length,\n",
        "                    padding = \"max_length\",\n",
        "                    truncation=True,\n",
        "                    return_tensors = 'pt',\n",
        "                )[\"input_ids\"][0]\n",
        "\n",
        "def make_torch_dataset(df, text_col, label_col, tokenizer):\n",
        "    \"\"\"\n",
        "    pandasのDataFrameで作成したデータをtokenizeしてpytorchのTensorDatasetへ変換\n",
        "\n",
        "    Args:\n",
        "        df (DataFrame): TensorDatasetへ変換するDataFrame\n",
        "        text_col (str): DataFrameの文章が格納されているカラム名\n",
        "        label_col (str): DataFrameの正解ラベルが格納されているカラム名\n",
        "        tokenizer (function): 使用するtokenizer\n",
        "\n",
        "    Returns:\n",
        "        TensorDataset\n",
        "    \"\"\"\n",
        "    label = df[label_col]\n",
        "    input_ids = []\n",
        "    for item in df[text_col].apply(tokenizer):\n",
        "        text = item.view(1, -1)\n",
        "        input_ids.append(text)\n",
        "        \n",
        "    ids = torch.cat(input_ids, dim=0)\n",
        "    label = torch.tensor(list(label))\n",
        "    \n",
        "    return TensorDataset(ids, label)\n",
        "\n",
        "\n",
        "def stratified_train_test_split_split(df,label_col,test_size=0.2):\n",
        "    \"\"\"\n",
        "    DataFrameを層化分割\n",
        "    \n",
        "    Args:\n",
        "        df (DataFrame): DataFrameの正解ラベルが格納されているカラム名\n",
        "        label_col (str): DataFrameの正解ラベルが格納されているカラム名\n",
        "        test_size (float, optional): testの割合. Defaults to 0.2.\n",
        "\n",
        "    Returns:\n",
        "        DataFrame\n",
        "    \"\"\"\n",
        "    df.reset_index(drop=True,inplace=True)\n",
        "    train, eval = train_test_split(df,stratify=df[label_col],random_state=0,test_size=0.2)\n",
        "    train_df = df.loc[train.index]\n",
        "    eval_df = df.loc[eval.index]\n",
        "    return train_df, eval_df\n",
        "    \n",
        "\n",
        "def train_model(net, dl_train, dl_eval, device, criterion, optimizer, patience=3, batch_size=16, n_epochs=20):\n",
        "    \"\"\"\n",
        "    early stoppingを使用したモデルの学習。\n",
        "\n",
        "    Args:\n",
        "        net (model): 学習の元となるモデル\n",
        "        dl_train (DataLoader): train用のDataLoader\n",
        "        dl_eval (DataLoader): eval用のDataLoader\n",
        "        device (device): 使用するdevice\n",
        "        criterion (criterion): 使用する損失関数\n",
        "        optimizer (optim): 使用するoptimizer\n",
        "        patience (int, optional): 指定のepoch数、指標が改善しなければearly stopping. Defaults to 3.\n",
        "        batch_size (int, optional): バッチサイズ. Defaults to 16.\n",
        "        n_epochs (int, optional): エポック数. Defaults to 20.\n",
        "\n",
        "    Returns:\n",
        "        model\n",
        "    \"\"\"    \n",
        "\n",
        "    net.to(device)\n",
        "    batch_size = dl_train.batch_size\n",
        "\n",
        "    train_losses = []\n",
        "    valid_losses = []\n",
        "    avg_train_losses = []\n",
        "    avg_valid_losses = [] \n",
        "    \n",
        "    early_stopping = EarlyStopping(patience=patience, delta=0.005, verbose=True)\n",
        "    for epoch in tqdm(range(1, n_epochs + 1),total=n_epochs,position=0 ,leave=True, desc=\"train\"):\n",
        "        net.train()\n",
        "        for batch in dl_train:\n",
        "            data = batch[0].to(device)  # 文章\n",
        "            target = batch[1].to(device)  # ラベル\n",
        "            optimizer.zero_grad()\n",
        "            output = net(data)[0]\n",
        "            loss = criterion(output, target)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            train_losses.append(loss.item())\n",
        "\n",
        "        net.eval()\n",
        "        for batch in dl_eval:\n",
        "            data = batch[0].to(device)  # 文章\n",
        "            target = batch[1].to(device)  # ラベル\n",
        "            output = net(data)[0]\n",
        "            loss = criterion(output, target)\n",
        "            valid_losses.append(loss.item())\n",
        "\n",
        "        train_loss = np.average(train_losses)\n",
        "        valid_loss = np.average(valid_losses)\n",
        "        avg_train_losses.append(train_loss)\n",
        "        avg_valid_losses.append(valid_loss)\n",
        "        \n",
        "        epoch_len = len(str(n_epochs))\n",
        "        \n",
        "        print_msg = (f'[{epoch:>{epoch_len}}/{n_epochs:>{epoch_len}}] ' +\n",
        "                     f'train_loss: {train_loss:.5f} ' +\n",
        "                     f'valid_loss: {valid_loss:.5f}')\n",
        "        \n",
        "        print(print_msg)\n",
        "        \n",
        "        train_losses = []\n",
        "        valid_losses = []\n",
        "        \n",
        "        early_stopping(valid_loss, net)\n",
        "        \n",
        "        if early_stopping.early_stop:\n",
        "            print(\"Early stopping\")\n",
        "            break\n",
        "        \n",
        "    net.load_state_dict(torch.load('checkpoint.pt'))\n",
        "\n",
        "    return net\n",
        "\n",
        "def valid_test(net_trained, device, dl_test):\n",
        "    \"\"\"\n",
        "    testデータへ当てはめ、accuracyの算出\n",
        "\n",
        "    Args:\n",
        "        net_trained (model): 学習済みモデル\n",
        "        device (device): 使用するdevice\n",
        "        dl_test (DataLoader): test用のDataLoader\n",
        "\n",
        "    Returns:\n",
        "        float: testデータのaccuracy\n",
        "    \"\"\"    \n",
        "    net_trained.eval()\n",
        "    net_trained.to(device)\n",
        "    epoch_corrects = 0\n",
        "\n",
        "    for batch in tqdm(dl_test,position=0 ,leave=True,desc=\"predict\"):\n",
        "        data = batch[0].to(device)  # 文章\n",
        "        target = batch[1].to(device)  # ラベル\n",
        "\n",
        "        with torch.set_grad_enabled(False):\n",
        "            outputs = net_trained(data)[0]\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            epoch_corrects += torch.sum(preds == target)\n",
        "\n",
        "    epoch_acc = epoch_corrects.double() / len(dl_test.dataset)\n",
        "    return epoch_acc"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I7GQqgSurotD"
      },
      "source": [
        "# 実行用\n",
        "def modeling(train_eval_df,test_df,num_epochs=20):\n",
        "    \n",
        "    # GPU設定\n",
        "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    # モデリング用データをtrainとevalに層化分割\n",
        "    train_df, eval_df = stratified_train_test_split_split(train_eval_df,label_col=\"label_index\")\n",
        "\n",
        "    # datasetに変換\n",
        "    dataset_train = make_torch_dataset(train_df, \"text\", \"label_index\", tokenizer_512)\n",
        "    dataset_eval = make_torch_dataset(eval_df, \"text\", \"label_index\", tokenizer_512)\n",
        "    dataset_test = make_torch_dataset(test_df, \"text\", \"label_index\", tokenizer_512)\n",
        "\n",
        "    # dataloader作成\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.manual_seed(1)\n",
        "    dl_train = DataLoader(dataset_train,batch_size = batch_size)\n",
        "    dl_eval = DataLoader(dataset_eval,batch_size = batch_size)\n",
        "    dl_test = DataLoader(dataset_test,batch_size = batch_size)\n",
        "    \n",
        "    # モデル構築\n",
        "    net = BertForSequenceClassification.from_pretrained(pretrained_model, num_labels=9)\n",
        "    net.train()\n",
        "\n",
        "    # 重みを変更する個所\n",
        "    for param in net.parameters():\n",
        "        param.requires_grad = False\n",
        "    for param in net.bert.encoder.layer[-1].parameters():\n",
        "        param.requires_grad = True\n",
        "    for param in net.classifier.parameters():\n",
        "        param.requires_grad = True\n",
        "\n",
        "    # 最適化手法\n",
        "    optimizer = optim.Adam([\n",
        "        {'params': net.bert.encoder.layer[-1].parameters(), 'lr': 5e-5},\n",
        "        {'params': net.classifier.parameters(), 'lr': 1e-4}\n",
        "    ])\n",
        "\n",
        "    # 損失関数\n",
        "    criterion = nn.CrossEntropyLoss()      \n",
        "    \n",
        "    # 訓練実施\n",
        "    net_trained =  train_model(net, dl_train, dl_eval, device, criterion, optimizer)\n",
        "    \n",
        "    # テストデータで検証\n",
        "    epoch_acc = valid_test(net_trained, device, dl_test)\n",
        "    \n",
        "    return epoch_acc.item()"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "up_RddXGI1F6"
      },
      "source": [
        "# モデリング、検証"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q1yhW1HLP6HA"
      },
      "source": [
        "# パラメータの設定\n",
        "batch_size = 16\n",
        "max_length = 512\n",
        "\n",
        "# testデータの読み込み\n",
        "test_df = pd.read_pickle(\"/content/qiita_eda/data/test_df.pkl\")\n",
        "test_df.columns = [\"text\",\"label_index\"]\n",
        "\n",
        "# trainデータの設定\n",
        "pickles_path = [\n",
        "                \"/content/qiita_eda/data/train_eval_eda_2000_5_16_gzip.pkl\", # alpha=0.05で作成したデータセット\n",
        "                \"/content/qiita_eda/data/train_eval_eda_2000_10_16_gzip.pkl\" # alpha=0.10で作成したデータセット\n",
        "                ]                \n",
        "alpha_list = [5,10]\n",
        "sampling_list = [500, 1000]\n",
        "# sampling_list = [2000]\n",
        "num_agg_list = [0,1,4,8,16]"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-MDefSgZBC00"
      },
      "source": [
        "result = pd.DataFrame(columns=[\"sampling_n\",\"alpha\",\"num_agg\",\"accuracy\",\"size\"])\n",
        "\n",
        "# pickle読み込み\n",
        "for path,alpha in zip(pickles_path,alpha_list):\n",
        "    \n",
        "    train_eval_all = pd.read_pickle(path,compression=\"gzip\")\n",
        "    train_eval_all.reset_index(drop=True,inplace=True)\n",
        "\n",
        "    unique_text_ids = list(train_eval_all.text_id.unique())\n",
        "    raw_aug_id = train_eval_all.loc[train_eval_all.raw_flg==1,\"aug_id\"].unique()[0]\n",
        "\n",
        "    # 件数を絞る\n",
        "    for sampling_n in sampling_list:\n",
        "        \n",
        "        random.seed(0)\n",
        "        text_id_list = random.sample(unique_text_ids,sampling_n)\n",
        "        text_id_sampled = train_eval_all[train_eval_all.text_id.isin(text_id_list)]\n",
        "        \n",
        "        for i ,num_agg in enumerate(num_agg_list):\n",
        "            if alpha!=alpha_list[0] and num_agg==0:\n",
        "                continue\n",
        "            # n_augで絞る\n",
        "            aug_ids = list(text_id_sampled.loc[text_id_sampled.raw_flg==0,\"aug_id\"].unique())\n",
        "            random.seed(0)\n",
        "            aug_id_list = random.sample(aug_ids,num_agg)\n",
        "            aug_id_list.append(raw_aug_id)\n",
        "\n",
        "            train_eval_df = text_id_sampled.loc[text_id_sampled.aug_id.isin(aug_id_list),[\"text\",\"label_index\"]]\n",
        "            acc = modeling(train_eval_df,test_df)\n",
        "            idx = len(result)\n",
        "            result.loc[idx] = [sampling_n, alpha, num_agg, acc,len(train_eval_df)]\n",
        "            display(result)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MQAOYKH-L3cZ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}