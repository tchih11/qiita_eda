{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "make_eda_dataset.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tchih11/qiita_eda/blob/main/notebooks/02_make_eda_dataset.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0NMDWmH9wpj9",
        "outputId": "a0150208-4c4a-4118-c7e6-2426c54f9975"
      },
      "source": [
        "!git clone https://tchih11:@github.com/tchih11/qiita_eda.git\r\n",
        "%cd /content/qiita_eda"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'qiita_eda'...\n",
            "remote: Enumerating objects: 34, done.\u001b[K\n",
            "remote: Counting objects: 100% (34/34), done.\u001b[K\n",
            "remote: Compressing objects: 100% (29/29), done.\u001b[K\n",
            "remote: Total 34 (delta 10), reused 8 (delta 2), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (34/34), done.\n",
            "/content/qiita_eda\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4fIQCjnVyCvu"
      },
      "source": [
        "# 各種インストール"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zc9HFNQLcctd"
      },
      "source": [
        "%%capture \r\n",
        "!pip install mecab-python3\r\n",
        "!pip install ipadic\r\n",
        "\r\n",
        "import gzip\r\n",
        "import os\r\n",
        "import random\r\n",
        "import shutil\r\n",
        "import sqlite3\r\n",
        "from math import ceil\r\n",
        "\r\n",
        "import ipadic\r\n",
        "import MeCab\r\n",
        "import pandas as pd\r\n",
        "from tqdm import tqdm\r\n",
        "\r\n",
        "tqdm.pandas()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5xw4PAazyOVC"
      },
      "source": [
        "# 日本語WordNet、stop word の設定"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7kvam6CAfdZJ",
        "outputId": "37c901d0-2b65-4afe-b9ac-e47f86d4f73f"
      },
      "source": [
        "# 日本語wordnetをDLして解凍\r\n",
        "! wget \"http://compling.hss.ntu.edu.sg/wnja/data/1.1/wnjpn.db.gz\"  # 1~2分\r\n",
        "\r\n",
        "with gzip.open('wnjpn.db.gz', 'rb') as f_in:\r\n",
        "    with open('wnjpn.db', 'wb') as f_out:\r\n",
        "        shutil.copyfileobj(f_in, f_out)\r\n",
        "\r\n",
        "# synset(概念ID)とlemma(単語)の組み合わせDataFrameの作成\r\n",
        "conn = sqlite3.connect(\"wnjpn.db\")\r\n",
        "q = 'SELECT synset,lemma FROM sense,word USING (wordid) WHERE sense.lang=\"jpn\"'\r\n",
        "sense_word = pd.read_sql(q, conn)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-02-08 23:51:22--  http://compling.hss.ntu.edu.sg/wnja/data/1.1/wnjpn.db.gz\n",
            "Resolving compling.hss.ntu.edu.sg (compling.hss.ntu.edu.sg)... 155.69.255.27\n",
            "Connecting to compling.hss.ntu.edu.sg (compling.hss.ntu.edu.sg)|155.69.255.27|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 60390049 (58M) [application/x-gzip]\n",
            "Saving to: ‘wnjpn.db.gz’\n",
            "\n",
            "wnjpn.db.gz         100%[===================>]  57.59M  2.79MB/s    in 20s     \n",
            "\n",
            "2021-02-08 23:51:42 (2.86 MB/s) - ‘wnjpn.db.gz’ saved [60390049/60390049]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "36sI6RxFfrte",
        "outputId": "c82bbad9-f1dc-47d6-c293-c44a8ef1f088"
      },
      "source": [
        "# stop words\r\n",
        "! wget \"http://svn.sourceforge.jp/svnroot/slothlib/CSharp/Version1/SlothLib/NLP/Filter/StopWord/word/Japanese.txt\"\r\n",
        "stop_words = pd.read_csv(\"Japanese.txt\",header=None)[0].to_list()        "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-02-08 23:51:44--  http://svn.sourceforge.jp/svnroot/slothlib/CSharp/Version1/SlothLib/NLP/Filter/StopWord/word/Japanese.txt\n",
            "Resolving svn.sourceforge.jp (svn.sourceforge.jp)... 44.240.209.230\n",
            "Connecting to svn.sourceforge.jp (svn.sourceforge.jp)|44.240.209.230|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2202 (2.2K) [text/plain]\n",
            "Saving to: ‘Japanese.txt’\n",
            "\n",
            "Japanese.txt        100%[===================>]   2.15K  --.-KB/s    in 0s      \n",
            "\n",
            "2021-02-08 23:51:44 (415 MB/s) - ‘Japanese.txt’ saved [2202/2202]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AukgooAFyHPh"
      },
      "source": [
        "# # EDA（Easy Data Augmentation）用の関数の定義"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XmHNxzdsLuJp"
      },
      "source": [
        "# 類義語をリストにして返す\r\n",
        "def get_synonyms(word):\r\n",
        "    \"\"\"\r\n",
        "    入力した単語の類似語を日本語wordnetから検索してリスト化\r\n",
        "\r\n",
        "    Args:\r\n",
        "        word (str): 類似語を検索する単語\r\n",
        "\r\n",
        "    Returns:\r\n",
        "        list: 入力した単語の類似語\r\n",
        "    \"\"\"    \r\n",
        "    synsets = sense_word.loc[sense_word.lemma == word, \"synset\"]\r\n",
        "    synset_words = set(sense_word.loc[sense_word.synset.isin(synsets), \"lemma\"])\r\n",
        "\r\n",
        "    if word in synset_words:\r\n",
        "        synset_words.remove(word)\r\n",
        "\r\n",
        "    return list(synset_words)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1jEB_GbDLt9g"
      },
      "source": [
        "# 分かち書き\r\n",
        "def wakati_text(text, hinshi=['名詞', '動詞']):\r\n",
        "    \"\"\"\r\n",
        "    分かち書き後のリストと同義語検索用の単語の原型リストを返す\r\n",
        "\r\n",
        "    Args:\r\n",
        "        text (str): 分かち書きする文章\r\n",
        "        hinshi (list, optional): 原型を取得する品詞. Defaults to ['名詞', '動詞'].\r\n",
        "\r\n",
        "    Returns:\r\n",
        "        list: 分かち書き後の単語、指定の品詞に絞った単語の原型リスト\r\n",
        "    \"\"\"    \r\n",
        "    m = MeCab.Tagger(ipadic.MECAB_ARGS)\r\n",
        "    p = m.parse(text)\r\n",
        "    p_split = [i.split(\"\\t\") for i in p.split(\"\\n\")][:-2]\r\n",
        "\r\n",
        "    # 原文の分かち書き\r\n",
        "    raw_words = [x[0] for x in p_split]\r\n",
        "\r\n",
        "    # 同義語検索用の単語の原型リスト（品詞を絞る）\r\n",
        "    second_half = [x[1].split(\",\") for x in p_split]\r\n",
        "    original_words = [x[6] if x[0] in hinshi else \"\" for x in second_half]\r\n",
        "    original_words = [\"\" if word in stop_words else word for word in original_words]\r\n",
        "\r\n",
        "    return raw_words, original_words"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fS6ZsiUXf4EU"
      },
      "source": [
        "\r\n",
        "def synonym_replacement(raw_words, original_words, n):\r\n",
        "    \"\"\"\r\n",
        "    文章の単語をランダムにn個同義語で置き換える\r\n",
        "\r\n",
        "    Args:\r\n",
        "        raw_words (list): 分かち書き済みのリスト\r\n",
        "        original_words (list): 単語の原型のリスト\r\n",
        "        n (int): 単語を置き換える件数\r\n",
        "\r\n",
        "    Returns:\r\n",
        "        list\r\n",
        "    \"\"\"    \r\n",
        "    new_words = raw_words.copy()\r\n",
        "\r\n",
        "    # 同義語に置き換える単語をランダムに決める\r\n",
        "    original_words_idx = [i for i, x in enumerate(original_words) if x != \"\"]\r\n",
        "    random.shuffle(original_words_idx)\r\n",
        "\r\n",
        "    # 指定の件数になるまで置き換え\r\n",
        "    num_replaced = 0\r\n",
        "    for idx in original_words_idx:\r\n",
        "        raw_word = raw_words[idx]\r\n",
        "        random_word = original_words[idx]\r\n",
        "        synonyms = get_synonyms(random_word)\r\n",
        "        if len(synonyms) >= 1:\r\n",
        "            synonym = random.choice(synonyms)\r\n",
        "            new_words = [synonym if word == raw_word else word for word in new_words]\r\n",
        "            num_replaced += 1\r\n",
        "        if num_replaced >= n:\r\n",
        "            break\r\n",
        "\r\n",
        "    return new_words\r\n",
        "\r\n",
        "def random_insertion(raw_words, original_words, n):\r\n",
        "    \"\"\"\r\n",
        "    文章の中にランダムに単語をn個挿入\r\n",
        "\r\n",
        "    Args:\r\n",
        "        raw_words (list): 分かち書き済みのリスト\r\n",
        "        original_words (list): 単語の原型のリスト\r\n",
        "        n (int): 挿入する単語数\r\n",
        "\r\n",
        "    Returns:\r\n",
        "        list\r\n",
        "    \"\"\"    \r\n",
        "    new_words = raw_words.copy()\r\n",
        "    for _ in range(n):\r\n",
        "        add_word(new_words, original_words)\r\n",
        "    return new_words\r\n",
        "\r\n",
        "def add_word(new_words, original_words):\r\n",
        "    synonyms = []\r\n",
        "    counter = 0\r\n",
        "    insert_word_original = [x for x in original_words if x]\r\n",
        "    while len(synonyms) < 1:\r\n",
        "        random_word = insert_word_original[random.randint(0, len(insert_word_original)-1)]\r\n",
        "        synonyms = get_synonyms(random_word)\r\n",
        "        counter += 1\r\n",
        "        if counter >= 10:\r\n",
        "            return\r\n",
        "    random_synonym = synonyms[0]\r\n",
        "    random_idx = random.randint(0, len(new_words)-1)\r\n",
        "    new_words.insert(random_idx, random_synonym)\r\n",
        "\r\n",
        "\r\n",
        "def random_deletion(words, p):\r\n",
        "    \"\"\"\r\n",
        "    文章の各単語を確率pで削除する\r\n",
        "\r\n",
        "    Args:\r\n",
        "        words (list): 分かち書き済みのリスト\r\n",
        "        p (float): 削除する確率\r\n",
        "\r\n",
        "    Returns:\r\n",
        "        list\r\n",
        "    \"\"\"    \r\n",
        "    # 1文字しかなければ削除しない\r\n",
        "    if len(words) == 1:\r\n",
        "        return words\r\n",
        "\r\n",
        "    # 確率pでランダムに削除\r\n",
        "    new_words = []\r\n",
        "    for word in words:\r\n",
        "        r = random.uniform(0, 1)\r\n",
        "        if r > p:\r\n",
        "            new_words.append(word)\r\n",
        "\r\n",
        "    # 全て削除してしまったら、ランダムに1つ単語を返す\r\n",
        "    if len(new_words) == 0:\r\n",
        "        rand_int = random.randint(0, len(words)-1)\r\n",
        "        return [words[rand_int]]\r\n",
        "\r\n",
        "    return new_words\r\n",
        "\r\n",
        "def random_swap(words, n):\r\n",
        "    \"\"\"\r\n",
        "    文章の単語の場所をn回入れ替える\r\n",
        "\r\n",
        "    Args:\r\n",
        "        words (list): 分かち書き済みのリスト\r\n",
        "        n (int): 入れ替える回数\r\n",
        "\r\n",
        "    Returns:\r\n",
        "        list\r\n",
        "    \"\"\"    \r\n",
        "    new_words = words.copy()\r\n",
        "    for _ in range(n):\r\n",
        "        nwords = swap_word(new_words)\r\n",
        "\r\n",
        "    return new_words\r\n",
        "\r\n",
        "def swap_word(new_words):\r\n",
        "    random_idx_1 = random.randint(0, len(new_words)-1)\r\n",
        "    random_idx_2 = random_idx_1\r\n",
        "    counter = 0\r\n",
        "    while random_idx_2 == random_idx_1:\r\n",
        "        random_idx_2 = random.randint(0, len(new_words)-1)\r\n",
        "        counter += 1\r\n",
        "        if counter > 3:\r\n",
        "            return new_words\r\n",
        "    new_words[random_idx_1], new_words[random_idx_2] = new_words[random_idx_2], new_words[random_idx_1]\r\n",
        "\r\n",
        "    return new_words\r\n",
        "\r\n",
        "# 各手法をまとめて実行\r\n",
        "def eda(sentence, alpha_sr=0.1, alpha_ri=0.1, alpha_rs=0.1, p_rd=0.1, num_aug=9):\r\n",
        "    \"\"\"\r\n",
        "    EDAの各手法をまとめて実行して、指定の件数分のEDA済み類似文章+原文をリストで返す。\r\n",
        "    原文はリストの最後に挿入される。\r\n",
        "\r\n",
        "    Args:\r\n",
        "        sentence (str): EDAを実行する文章（原文）\r\n",
        "        alpha_sr (float, optional): synonym_replacementのalpha. Defaults to 0.1.\r\n",
        "        alpha_ri (float, optional): random_insertionのalpha. Defaults to 0.1.\r\n",
        "        alpha_rs (float, optional): random_swapのalpha. Defaults to 0.1.\r\n",
        "        p_rd (float, optional): random_deletionのalpha. Defaults to 0.1.\r\n",
        "        num_aug (int, optional): EDAで作成する文章数. Defaults to 9.\r\n",
        "\r\n",
        "    Returns:\r\n",
        "        [type]: [description]\r\n",
        "    \"\"\"    \r\n",
        "\r\n",
        "    # 分かち書き\r\n",
        "    raw_words, original_words = wakati_text(sentence)\r\n",
        "    num_words = len(raw_words)\r\n",
        "\r\n",
        "    augmented_sentences = []\r\n",
        "    techniques = ceil(alpha_sr) + ceil(alpha_ri) + ceil(alpha_rs) + ceil(p_rd)\r\n",
        "    if techniques == 0:\r\n",
        "        return\r\n",
        "\r\n",
        "    num_new_per_technique = int(num_aug/techniques)+1\r\n",
        "\r\n",
        "    #ランダムに単語を同義語でn個置き換える\r\n",
        "    if (alpha_sr > 0):\r\n",
        "        n_sr = max(1, int(alpha_sr*num_words))\r\n",
        "        for _ in range(num_new_per_technique):\r\n",
        "            a_words = synonym_replacement(raw_words,original_words ,n_sr)\r\n",
        "            augmented_sentences.append(''.join(a_words))\r\n",
        "\r\n",
        "    #ランダムに文中に出現する単語の同義語をn個挿入\r\n",
        "    if (alpha_ri > 0):\r\n",
        "        n_ri = max(1, int(alpha_ri*num_words))\r\n",
        "        for _ in range(num_new_per_technique):\r\n",
        "            a_words = random_insertion(raw_words,original_words, n_ri)\r\n",
        "            augmented_sentences.append(''.join(a_words))\r\n",
        "\r\n",
        "    #ランダムに単語の場所をn回入れ替える\r\n",
        "    if (alpha_rs > 0):\r\n",
        "        n_rs = max(1, int(alpha_rs*num_words))\r\n",
        "        for _ in range(num_new_per_technique):\r\n",
        "            a_words = random_swap(raw_words, n_rs)\r\n",
        "            augmented_sentences.append(''.join(a_words))\r\n",
        "\r\n",
        "    #ランダムに単語を確率pで削除する\r\n",
        "    if (p_rd > 0):\r\n",
        "        for _ in range(num_new_per_technique):\r\n",
        "            a_words = random_deletion(raw_words, p_rd)\r\n",
        "            augmented_sentences.append(''.join(a_words))\r\n",
        "\r\n",
        "    #必要な文章の数だけランダムに抽出\r\n",
        "    random.shuffle(augmented_sentences)\r\n",
        "    augmented_sentences = augmented_sentences[:num_aug]\r\n",
        "\r\n",
        "    #原文もリストに加える\r\n",
        "    augmented_sentences.append(sentence)\r\n",
        "\r\n",
        "    return augmented_sentences"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hElE2YM3ymvy"
      },
      "source": [
        "# EDAの実行\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kzpvzmxxUEaE"
      },
      "source": [
        "# パラメータとサンプリング件数を指定してEDAを実行する関数\r\n",
        "\r\n",
        "def make_eda_datasets(sample_n, num_aug, alpha, save_dir=None):\r\n",
        "\r\n",
        "    # 元データ\r\n",
        "    train_eval = pd.read_pickle(\"./data/train_eval_df.pkl\")\r\n",
        "    train_eval.columns = [\"text\", \"label_index\"]\r\n",
        "\r\n",
        "    # 指定の数にサンプリング\r\n",
        "    train_eval_sampled = train_eval.sample(n=sample_n, random_state=0)\r\n",
        "    train_eval_sampled.reset_index(drop=True)\r\n",
        "    \r\n",
        "    # edaを実行\r\n",
        "    train_eval_sampled[\"text\"] = train_eval_sampled.text.progress_apply(lambda x: eda(\r\n",
        "        x, alpha_sr=alpha, alpha_ri=alpha, alpha_rs=alpha, p_rd=alpha, num_aug=num_aug))\r\n",
        "    # dataframeのセル内にリストができるので展開\r\n",
        "    train_eval_sampled = train_eval_sampled.explode(\"text\")\r\n",
        "\r\n",
        "    # idの付与\r\n",
        "    train_eval_sampled[\"text_id\"] = train_eval_sampled.index\r\n",
        "    train_eval_sampled[\"aug_id\"] = train_eval_sampled.groupby(\"text_id\").cumcount()\r\n",
        "    \r\n",
        "    # eda前の原文は各text_idの最後にが入っているのでフラグを立てる\r\n",
        "    train_eval_sampled[\"raw_flg\"] = (train_eval_sampled.aug_id == num_aug)*1\r\n",
        "    train_eval_sampled.reset_index(drop=True)\r\n",
        "\r\n",
        "    # 出力先の指定がある場合はpickleで保存\r\n",
        "    if save_dir:\r\n",
        "        train_eval_sampled.to_pickle(\r\n",
        "            f\"{save_dir}/train_eval_eda_{sample_n}_{int(alpha*100)}_{num_aug}_gzip.pkl\", compression=\"gzip\")\r\n",
        "\r\n",
        "    return train_eval_sampled"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rnQxodfnPpR1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "53e6234c-e444-4deb-dda4-b1cda58ce2f3"
      },
      "source": [
        "# 今回の検証では、alpha=[0.05,0.10]を試す\r\n",
        "# sample_n、num_augはそれぞれ検証する際の最大値である2000,16に作成しておいて、モデリング時に削る\r\n",
        "\r\n",
        "%%time\r\n",
        "save_dir = \"/content/qiita_eda/data\"\r\n",
        "\r\n",
        "# 12時間ルールにひっかかるので、コメントアウトしながら1つずつ実行\r\n",
        "# eda_2000_5_16 = make_eda_datasets(sample_n=2000,alpha=0.05,num_aug=16,save_dir=save_dir)\r\n",
        "eda_2000_10_16 = make_eda_datasets(sample_n=2000,alpha=0.1,num_aug=16,save_dir=save_dir)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 2000/2000 [9:34:12<00:00, 17.23s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 9h 33min 4s, sys: 10.7 s, total: 9h 33min 14s\n",
            "Wall time: 9h 34min 12s\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    }
  ]
}